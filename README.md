# Accelerating-GNNs-via-Knowledge-Distillation
Developed a dual-framework approach for compressing Graph Neural Networks using supervised distillation. Achieved up to 75\% parameter reduction on small graphs (Cora) and 62\% on large-scale graphs (ogbn-arxiv) with &lt;3\% accuracy loss. Integrated multi-component loss design, projection networks, and pruning to enable efficient model deployment.
