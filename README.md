# Accelerating-GNNs-via-Knowledge-Distillation
Developed in collaboration with Dhruvil Gorasiya and Yogesh Kulkarni a dual-framework approach for compressing Graph Neural Networks using supervised distillation. Achieved up to 75\% parameter reduction on small graphs (Cora) and 62\% on large-scale graphs (ogbn-arxiv) with &lt;3\% accuracy loss. Integrated multi-component loss design, projection networks, and pruning to enable efficient model deployment.
